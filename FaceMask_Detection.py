# -*- coding: utf-8 -*-
"""TryShitML

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z9wlSZxFO2HzTJxbEXF4-RDgsPJfqLHR
"""

import numpy as np
import PIL
from PIL import Image
import cv2
from google.colab.patches import cv2_imshow
import matplotlib.pyplot as plt

withMask = []
withoutMask = []

from google.colab import drive
drive.mount('/content/drive')

import glob 
import shutil

def createMaskList():
  tmp_list = []
  pattern = "/content/drive/MyDrive/AFDB_masked_face_dataset/*/*"  
  for img in glob.glob(pattern):
      tmp_list.append(str(img))
  return tmp_list

withMask = createMaskList();

print(len(withMask))
print(withMask[0])

import glob 
import shutil

def createNoMaskList():
  count = 0
  tmp_list = []   
  pattern = "/content/drive/MyDrive/AFDB_face_dataset/*/*"  
  for img in glob.glob(pattern):
    if (count == 4500):
        break
    tmp_list.append(str(img))
    count += 1
  return tmp_list

withoutMask = createNoMaskList();

print(len(withoutMask))

def createDatabase(mask, no_mask):
  dataBase = []
  label = []
  for path in mask:
    img = cv2.imread(path)
    gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
    resized = cv2.resize(gray_img,(64, 64), interpolation=cv2.INTER_CUBIC)
    dataBase.append(resized.flatten())
    label.append(1)
    
  
  for path in no_mask:
    img = cv2.imread(path)
    gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
    resized = cv2.resize(gray_img,(64, 64), interpolation=cv2.INTER_CUBIC)
    dataBase.append(resized.flatten())
    label.append(0)
  
  return dataBase, label

mask_dataset, mask_label = createDatabase(withMask, withoutMask)

print(len(mask_dataset))
print(len(mask_dataset[0]))
print(len(mask_label))

"""##Scaling"""

from sklearn.preprocessing import StandardScaler  
scaler = StandardScaler()  

scaler.fit(mask_dataset)  
mask_dataset_Std = scaler.transform(mask_dataset)

"""##PCA"""

from sklearn.decomposition import PCA
pca = PCA(0.99)
mask_dataset_pca = pca.fit_transform(mask_dataset_Std)

pca.explained_variance_ratio_

pca.n_components_

"""##LDA"""

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

lda = LinearDiscriminantAnalysis()
mask_dataset_lda = lda.fit_transform(mask_dataset_Std, mask_label)

lda.explained_variance_ratio_

"""##Shuffling"""

from sklearn.utils import shuffle

def Shuffle(array1, array2):
  array1_shuffled, array2_shuffled = shuffle(array1, array2)
  return array1_shuffled, array2_shuffled

"""##Train-Test-Split"""

from sklearn.model_selection import train_test_split

mask_dataset_shuffle, y = Shuffle(mask_dataset, mask_label)  

X_train, X_test, y_train, y_test = train_test_split(mask_dataset_shuffle, y, test_size=0.1)

mask_dataset_shuffle_pca, y_pca = Shuffle(mask_dataset_pca, mask_label)  

X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(mask_dataset_shuffle_pca, y_pca, test_size=0.1)

mask_dataset_shuffle_lda, y_lda = Shuffle(mask_dataset_lda, mask_label)  

X_train_lda, X_test_lda, y_train_lda, y_test_lda = train_test_split(mask_dataset_shuffle_lda, y_lda, test_size=0.1)

"""##Training"""

from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
from sklearn.metrics import log_loss
from sklearn.model_selection import cross_val_score

"""##SVC"""

from sklearn.svm import SVC
def trainSVC(train_data, label_data, test_data,test_label):
  model = SVC()
  
  model.fit(train_data,label_data)
  print(model.score(test_data,test_label))
  cvs = cross_val_score(model, train_data, label_data, cv=5)
  print(cvs)
  predicted_test = model.predict(test_data)
  return predicted_test, cvs

"""##SVC_Normal"""

predicted_SVC, cvs_SVC = trainSVC(X_train, y_train, X_test,  y_test)
predicted_SVC

accuracy_score(y_test, predicted_SVC)

f1_score(y_test, predicted_SVC)

log_loss(y_test, predicted_SVC)

"""##SVC_PCA"""

predicted_SVC_pca, cvs_SVC_pca = trainSVC(X_train_pca, y_train_pca, X_test_pca, y_test_pca)
predicted_SVC_pca

accuracy_score(y_test_pca, predicted_SVC_pca)

f1_score(y_test_pca, predicted_SVC_pca)

log_loss(y_test_pca, predicted_SVC_pca)

"""##SVC_LDA"""

predicted_SVC_lda, cvs_SVC_lda = trainSVC(X_train_lda, y_train_lda, X_test_lda, y_test_lda)
predicted_SVC_lda

accuracy_score(y_test_lda, predicted_SVC_lda)

f1_score(y_test_lda, predicted_SVC_lda)

log_loss(y_test_lda, predicted_SVC_lda)

"""##Comparison"""

#BoxPlot for cross validation scores

data_SVC = [cvs_SVC, cvs_SVC_pca,cvs_SVC_lda]

fig_SVC = plt.figure(figsize =(10, 7))
  
# Creating plot
plt.boxplot(data_SVC)
  
# show plot
plt.show()

"""##MLP"""

from sklearn.neural_network import MLPClassifier
def trainMLP(train_data, label_data, test_data,test_label):
  clf = MLPClassifier()
  clf=clf.fit(train_data,label_data)
  print(clf.score(test_data,test_label))
  cvs = cross_val_score(clf, train_data, label_data, cv=5)
  print(cvs)
  predicted_test = clf.predict(test_data)
  return predicted_test, cvs

"""##MLP_Normal"""

predicted_MLP, cvs_MLP = trainMLP(X_train, y_train, X_test,  y_test)
predicted_MLP

accuracy_score(y_test, predicted_MLP)

f1_score(y_test, predicted_MLP)

log_loss(y_test, predicted_MLP)

"""##MLP_PCA"""

predicted_MLP_pca, cvs_MLP_pca = trainMLP(X_train_pca, y_train_pca, X_test_pca, y_test_pca)
predicted_MLP_pca

accuracy_score(y_test_pca, predicted_MLP_pca)

f1_score(y_test_pca, predicted_MLP_pca)

log_loss(y_test_pca, predicted_MLP_pca)

"""##MLP_LDA"""

predicted_MLP_lda, cvs_MLP_lda = trainMLP(X_train_lda, y_train_lda, X_test_lda, y_test_lda)
predicted_MLP_lda

accuracy_score(y_test_lda, predicted_MLP_lda)

f1_score(y_test_lda, predicted_MLP_lda)

log_loss(y_test_lda, predicted_MLP_lda)

"""##Comaprison"""

#BoxPlot for cross validation scores

data_MLP = [cvs_MLP, cvs_MLP_pca,cvs_MLP_lda]

fig_MLP = plt.figure(figsize =(10, 7))
  
# Creating plot
plt.boxplot(data_MLP)
  
# show plot
plt.show()

"""##Random Forest"""

from sklearn.ensemble import RandomForestClassifier
def trainRandomForest(train_data, label_data, test_data,test_label):
  clf = RandomForestClassifier()
  clf=clf.fit(train_data,label_data)
  print(clf.score(test_data,test_label))
  cvs = cross_val_score(clf, train_data, label_data, cv=5)
  print(cvs)
  predicted_test = clf.predict(test_data)
  return predicted_test, cvs

"""##RandomForest_Normal"""

predicted_RF, cvs_RF = trainRandomForest(X_train, y_train, X_test,  y_test)
predicted_RF

accuracy_score(y_test, predicted_RF)

f1_score(y_test, predicted_RF)

log_loss(y_test, predicted_RF)

"""##RandomForest_PCA"""

predicted_RF_pca, cvs_RF_pca = trainRandomForest(X_train_pca, y_train_pca, X_test_pca, y_test_pca)
predicted_RF_pca

accuracy_score(y_test_pca, predicted_RF_pca)

f1_score(y_test_pca, predicted_RF_pca)

log_loss(y_test_pca, predicted_RF_pca)

"""##RandomForest_LDA"""

predicted_RF_lda, cvs_RF_lda = trainRandomForest(X_train_lda, y_train_lda, X_test_lda, y_test_lda)
predicted_RF_lda

accuracy_score(y_test_lda, predicted_RF_lda)

f1_score(y_test_lda, predicted_RF_lda)

log_loss(y_test_lda, predicted_RF_lda)

"""##Comparison"""

#BoxPlot for cross validation scores

data_RF = [cvs_RF, cvs_RF_pca,cvs_RF_lda]

fig_RF = plt.figure(figsize =(10, 7))
  
# Creating plot
plt.boxplot(data_RF)
  
# show plot
plt.show()

"""##CNN"""

from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import AveragePooling2D
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Input
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.applications.mobilenet_v2 import preprocess_input
from tensorflow.keras.preprocessing.image import img_to_array
from tensorflow.keras.preprocessing.image import load_img
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import LabelBinarizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from imutils import paths
import matplotlib.pyplot as plt
import numpy as np
import argparse
import os

INIT_LR = 1e-4
EPOCHS = 20
BS = 32

def createDatabaseCNN(mask, no_mask):
  dataBase = []
  label = []
  for path in mask:
    img = cv2.imread(path)
   
    resized = cv2.resize(img,(64, 64), interpolation=cv2.INTER_CUBIC)
    
    dataBase.append(resized)
    label.append(1)
    
  
  for path in no_mask:
    img = cv2.imread(path)
    resized = cv2.resize(img,(64, 64), interpolation=cv2.INTER_CUBIC)
    dataBase.append(resized)
    label.append(0)
  
  return dataBase, label

mask_dataset_cnn, mask_label_cnn = createDatabaseCNN(withMask, withoutMask)

print(mask_dataset_cnn)

mask_dataset_cnn= np.array(mask_dataset_cnn, dtype="float32")
mask_label_cnn = np.array(mask_label_cnn)

# perform one-hot encoding on the labels
lb = LabelBinarizer()
mask_label_cnn = lb.fit_transform(mask_label_cnn)
mask_label_cnn = to_categorical(mask_label_cnn)

(trainX, testX, trainY, testY) = train_test_split(mask_dataset_cnn, mask_label_cnn,
	test_size=0.20, stratify=mask_label_cnn, random_state=42)

aug = ImageDataGenerator(
	rotation_range=20,
	zoom_range=0.15,
	width_shift_range=0.2,
	height_shift_range=0.2,
	shear_range=0.15,
	horizontal_flip=True,
	fill_mode="nearest")

baseModel = MobileNetV2(weights="imagenet", include_top=False,
	input_tensor=Input(shape=(64, 64,3)))

first_cnn_model = baseModel.output
first_cnn_model = AveragePooling2D(pool_size=(2, 2))(first_cnn_model)
first_cnn_model = Flatten(name="flatten")(first_cnn_model)
first_cnn_model = Dense(128, activation="relu")(first_cnn_model)
first_cnn_model = Dropout(0.5)(first_cnn_model)
first_cnn_model = Dense(2, activation="softmax")(first_cnn_model)

model = Model(inputs=baseModel.input, outputs=first_cnn_model)

for layer in baseModel.layers:
	layer.trainable = False

opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)
model.compile(loss="binary_crossentropy", optimizer=opt,
	metrics=["accuracy"])

CNN = model.fit(
	aug.flow(trainX, trainY, batch_size=BS),
	steps_per_epoch=len(trainX) // BS,
	validation_data=(testX, testY),
	validation_steps=len(testX) // BS,
	epochs=EPOCHS)

predIdxs = model.predict(testX, batch_size=BS)

predIdxs = np.argmax(predIdxs, axis=1)

test_Y = np.argmax(testY, axis=1)

print(accuracy_score(test_Y,predIdxs))

test_Y_ = np.argmax(test_Y, axis=0)

print(classification_report(test_Y, predIdxs))

N = EPOCHS
plt.style.use("ggplot")
plt.figure()
plt.plot(np.arange(0, N), CNN.history["loss"], label="train_loss")
plt.plot(np.arange(0, N), CNN.history["val_loss"], label="val_loss")
plt.plot(np.arange(0, N), CNN.history["accuracy"], label="train_acc")
plt.plot(np.arange(0, N), CNN.history["val_accuracy"], label="val_acc")
plt.title("Training Loss and Accuracy")
plt.xlabel("Epoch #")
plt.ylabel("Loss/Accuracy")
plt.legend(loc="lower left")